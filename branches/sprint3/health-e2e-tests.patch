diff --git a/app/server.py b/app/server.py
index 129c4f7..c06fb6b 100644
--- a/app/server.py
+++ b/app/server.py
@@ -1,4 +1,4 @@
-from flask import Flask, Response
+from flask import Flask, Response, jsonify
 from datetime import datetime, timezone
 from zoneinfo import ZoneInfo
 from logger_service import start_background_logging, logger
@@ -19,6 +19,11 @@ def get_current_time():
     return Response(response_string, mimetype='text/plain')
 
 
+@app.route("/health")
+def health_check():
+    return jsonify({"status": "ok"})
+
+
 if __name__ == "__main__":
     start_background_logging()
     logger.info("Flask server starting on 0.0.0.0:80")
diff --git a/k8s/deploy.yaml b/k8s/deploy.yaml
index 098e2a4..e19f372 100644
--- a/k8s/deploy.yaml
+++ b/k8s/deploy.yaml
@@ -15,5 +15,5 @@ spec:
     spec:
       containers:
       - name: timeserver-container
-        image: timeserver:v2
+        image: timeserver:v3
         imagePullPolicy: Never
\ No newline at end of file
diff --git a/pytest.ini b/pytest.ini
index 03f586d..1ddd7f5 100644
--- a/pytest.ini
+++ b/pytest.ini
@@ -1,2 +1,11 @@
 [pytest]
-pythonpath = .
\ No newline at end of file
+pythonpath = .
+addopts = --cov=scripts --cov-report=term-missing --cov-report=html --cov-fail-under=80 -vv
+filterwarnings =
+    ignore::DeprecationWarning
+
+[coverage:run]
+source = scripts
+omit =
+    tests/*
+    */tests/*
\ No newline at end of file
diff --git a/tests/conftest.py b/tests/conftest.py
new file mode 100644
index 0000000..9f72d09
--- /dev/null
+++ b/tests/conftest.py
@@ -0,0 +1,151 @@
+import pytest
+from kubernetes import config, client
+import shutil
+from pathlib import Path
+from scripts.metric_collector import metric_collector
+from scripts.log_collector import log_collector
+from scripts.metric_collector import metric_visualizer
+import pandas as pd
+
+NAMESPACE = "default"
+DEPLOYMENT_NAME = "timeserver"
+SERVICE_NAME = "timeserver"
+POD_LABEL_SELECTOR = "pod=timeserver-pod"
+
+
+@pytest.fixture(scope="session")
+def resource_names():
+    return {
+        "namespace": NAMESPACE,
+        "deployment": DEPLOYMENT_NAME,
+        "service": SERVICE_NAME,
+        "pod_label_selector": POD_LABEL_SELECTOR
+    }
+
+
+@pytest.fixture(scope="session")
+def project_root():
+    return Path(__file__).parent.parent
+
+
+@pytest.fixture(scope="session")
+def app_service_url():
+    return "http://localhost:80"
+
+
+@pytest.fixture(scope="session")
+def kube_api_client():
+    try:
+        config.load_incluster_config()
+    except config.ConfigException:
+        config.load_kube_config()
+    return client.AppsV1Api()
+
+
+@pytest.fixture(scope="session")
+def kube_core_api_client():
+    try:
+        config.load_incluster_config()
+    except config.ConfigException:
+        config.load_kube_config()
+    return client.CoreV1Api()
+
+
+@pytest.fixture(scope="module")
+def timeserver_deployment(kube_api_client: client.AppsV1Api):
+    try:
+        return kube_api_client.read_namespaced_deployment(
+            name=DEPLOYMENT_NAME, namespace=NAMESPACE
+        )
+    except Exception as e:
+        pytest.fail(f"Fallo al obtener el Deployment '{DEPLOYMENT_NAME}'. Error: {e}")
+
+
+@pytest.fixture(scope="module")
+def timeserver_service(kube_core_api_client: client.CoreV1Api):
+    try:
+        return kube_core_api_client.read_namespaced_service(
+            name=SERVICE_NAME, namespace=NAMESPACE
+        )
+    except Exception as e:
+        pytest.fail(f"Fallo al encontrar el Service '{SERVICE_NAME}'. Error: {e}")
+
+
+@pytest.fixture(scope="module")
+def timeserver_pods(kube_core_api_client: client.CoreV1Api):
+    try:
+        pods = kube_core_api_client.list_namespaced_pod(
+            namespace=NAMESPACE, label_selector=POD_LABEL_SELECTOR
+        )
+        return pods.items
+    except Exception as e:
+        pytest.fail(f"Fallo al listar los pods. Error: {e}")
+
+
+@pytest.fixture(scope="function")
+def observability_dirs(project_root):
+    dirs = {
+        "metrics": project_root / "metrics",
+        "logs": project_root / "logs",
+        "alerts": project_root / "alerts"
+    }
+    for path in dirs.values():
+        if path.exists():
+            shutil.rmtree(path)
+        path.mkdir(parents=True, exist_ok=True)
+    yield dirs
+    for path in dirs.values():
+        if path.exists():
+            shutil.rmtree(path)
+
+
+@pytest.fixture(scope="function")
+def run_metric_collector(observability_dirs):
+    try:
+        metric_collector.main()
+        return observability_dirs["metrics"]
+    except Exception as e:
+        pytest.fail(f"La recoleccion de metricas fallo: {e}")
+
+
+@pytest.fixture(scope="function")
+def run_log_collector(observability_dirs, timeserver_pods):
+    if not timeserver_pods:
+        pytest.skip("No se encontraron pods para la recoleccion de logs.")
+
+    pod_names = [p.metadata.name for p in timeserver_pods]
+    try:
+        log_collector.collect_logs(pod_names, NAMESPACE)
+        return observability_dirs["logs"]
+    except Exception as e:
+        pytest.fail(f"La recoleccion de logs fallo: {e}")
+
+
+@pytest.fixture(scope="function")
+def run_metric_visualizer(run_metric_collector):
+    try:
+        metric_visualizer.main()
+        return run_metric_collector
+    except Exception as e:
+        pytest.fail(f"La visualización de métricas falló: {e}")
+
+
+@pytest.fixture
+def mock_metrics_dir(tmp_path):
+    metrics_path = tmp_path / "metrics"
+    pods_path = metrics_path / "pods"
+    pods_path.mkdir(parents=True)
+
+    pd.DataFrame({
+        'NAME': ['pod-1'],
+        'CPU(cores)': ['5m'],
+        'MEMORY(bytes)': ['10Mi']
+    }).to_csv(pods_path / "normal_metrics.csv", sep='\t', index=False)
+
+    pd.DataFrame({
+        'NAME': ['pod-2'],
+        'CPU(cores)': ['15m'],
+        'MEMORY(bytes)': ['20Mi']
+    }).to_csv(pods_path / "alert_metrics.csv", sep='\t', index=False)
+
+    return metrics_path
diff --git a/tests/test_e2e.py b/tests/test_e2e.py
new file mode 100644
index 0000000..6ab0ef9
--- /dev/null
+++ b/tests/test_e2e.py
@@ -0,0 +1,122 @@
+from unittest.mock import patch, MagicMock
+from scripts.metric_collector import metric_visualizer
+
+def test_metric_dirs(run_metric_collector):
+    metrics_dir = run_metric_collector
+    assert metrics_dir.exists(), "El directorio principal de métricas no fue creado."
+    pods_dir = metrics_dir / "pods"
+    nodes_dir = metrics_dir / "nodes"
+    assert pods_dir.exists(), "El directorio de métricas de pods no fue creado."
+    assert nodes_dir.exists(), "El directorio de métricas de nodos no fue creado."
+
+
+def test_metric_files(run_metric_collector):
+    metrics_dir = run_metric_collector
+
+    csv_files = list(metrics_dir.rglob("*.csv"))
+    json_files = list(metrics_dir.rglob("*.json"))
+
+    assert len(csv_files) > 0, "No se generaron archivos CSV de métricas."
+    assert len(json_files) > 0, "No se generaron archivos JSON de métricas."
+
+
+def test_log_files(run_log_collector):
+    logs_dir = run_log_collector
+    assert logs_dir.exists(), "El directorio de logs no fue creado."
+
+    main_log_file = logs_dir / "all_pods.log"
+    individual_logs = [f for f in logs_dir.glob("*.log") if f.name != "all_pods.log"]
+
+    assert main_log_file.exists(), "El archivo de log principal no fue creado."
+    assert len(individual_logs) > 0, "No se crearon logs individuales para los pods."
+
+
+def test_log_not_empty(run_log_collector):
+    main_log_file = run_log_collector / "all_pods.log"
+
+    with open(main_log_file, 'r', encoding='utf-8') as f:
+        content = f.read()
+        print(f"contenido {content}")
+
+    assert len(content) > 0, "El archivo de logs principal está vacío."
+    assert "Logs del pod:" in content, "El formato esperado no se encontró en el log."
+
+
+def test_html_graphs_generated(run_metric_visualizer):
+    metrics_dir = run_metric_visualizer
+    html_files = list(metrics_dir.rglob("*.html"))
+    assert len(html_files) > 0, "No se genero HTML archivo"
+
+
+def test_html_graphs_not_empty(run_metric_visualizer):
+    metrics_dir = run_metric_visualizer
+    html_files = list(metrics_dir.rglob("*.html"))
+
+    for html_file in html_files:
+        with open(html_file, 'r', encoding='utf-8') as f:
+            content = f.read()
+        assert len(content) > 0, f"El archivo HTML {html_file} esta vacío."
+        assert "<html>" in content, f"El archivo {html_file} no es un HTML valido."
+
+def test_alert_umbral(monkeypatch, capsys, mock_metrics_dir):
+    monkeypatch.setattr(metric_visualizer, 'metrics_dir', mock_metrics_dir)
+    metric_visualizer.alert_umbral()
+    captured = capsys.readouterr()
+    assert "15m de CPU" in captured.out
+    assert "pod-1" not in captured.out
+
+
+@patch('subprocess.run')
+def test_alert_pods_not_ready(mock_subprocess_run, capsys):
+    not_ready_pod_json = '''
+    {
+        "items": [
+            {
+                "metadata": {
+                    "name": "test-pod-not-ready"
+                },
+                "status": {
+                    "conditions": [
+                        {
+                            "type": "Ready",
+                            "status": "False",
+                            "lastTransitionTime": "2025-01-01T12:00:00Z"
+                        }
+                    ]
+                }
+            }
+        ]
+    }
+    '''
+
+    ready_pod_json = '''
+    {
+        "items": [
+            {
+                "metadata": {
+                    "name": "test-pod-ready"
+                },
+                "status": {
+                    "conditions": [
+                        {
+                            "type": "Ready",
+                            "status": "True"
+                        }
+                    ]
+                }
+            }
+        ]
+    }
+    '''
+
+    mock_subprocess_run.side_effect = [
+        MagicMock(stdout=not_ready_pod_json, check_return_value=None),
+        MagicMock(stdout=ready_pod_json, check_return_value=None)
+    ]
+
+    namespaces = ["ns1", "ns2"]
+    metric_visualizer.alert_pods_not_ready(namespaces)
+
+    captured = capsys.readouterr()
+    assert "no está Ready" in captured.out
+    assert "test-pod-ready" not in captured.out
diff --git a/tests/test_health_api.py b/tests/test_health_api.py
new file mode 100644
index 0000000..df69515
--- /dev/null
+++ b/tests/test_health_api.py
@@ -0,0 +1,48 @@
+import requests
+import pytest
+import time
+
+
+def test_health_ok(app_service_url: str):
+    health_check_url = f"{app_service_url}/health"
+    try:
+        response = requests.get(health_check_url, timeout=5)
+        response.raise_for_status()
+
+        assert response.json() == {"status": "ok"}, \
+            "El cuerpo de la respuesta de /health no es el esperado."
+
+    except requests.exceptions.RequestException as e:
+        pytest.fail(f"No se pudo conectar con el endpoint /health. Error: {e}")
+
+
+def test_content_endpoint(app_service_url: str):
+    try:
+        response = requests.get(app_service_url, timeout=5)
+        response.raise_for_status()
+
+        response_text = response.text
+        assert "UTC" in response_text, "La respuesta no contiene la hora en UTC."
+        assert "Lima" in response_text, "La respuesta no contiene la hora de Lima."
+
+    except requests.exceptions.RequestException as e:
+        pytest.fail(f"No se pudo conectar con el endpoint principal ('/'). Error: {e}")
+
+
+def test_health_performance(app_service_url: str):
+    health_check_url = f"{app_service_url}/health"
+
+    try:
+        start_time = time.time()
+        response = requests.get(health_check_url, timeout=5)
+        end_time = time.time()
+
+        response.raise_for_status()
+
+        response_time = end_time - start_time
+
+        assert response_time < 1.0, \
+            f"El tiempo de respuesta del health check es demasiado alto: {response_time:.2f}s"
+
+    except requests.exceptions.RequestException as e:
+        pytest.fail(f"No se pudo conectar con el endpoint /health. Error: {e}")
diff --git a/tests/test_health_infra.py b/tests/test_health_infra.py
new file mode 100644
index 0000000..1370ffa
--- /dev/null
+++ b/tests/test_health_infra.py
@@ -0,0 +1,23 @@
+def test_deployment_available(timeserver_deployment):
+    status = timeserver_deployment.status
+    spec = timeserver_deployment.spec
+
+    assert status.available_replicas == spec.replicas, \
+        f"El numero de replicas disponibles ({status.available_replicas}) no coincide con el deseado ({spec.replicas})."
+
+    assert status.ready_replicas == spec.replicas, \
+        f"El numero de replicas listas ({status.ready_replicas}) no coincide con el deseado ({spec.replicas})."
+
+
+def test_service_exists(timeserver_service, resource_names):
+    assert timeserver_service is not None
+    assert timeserver_service.metadata.name == resource_names["service"]
+
+
+def test_pods_running(timeserver_pods, resource_names):
+    assert timeserver_pods is not None, "No se encontraron pods."
+
+    assert len(timeserver_pods) == 3
+
+    for pod in timeserver_pods:
+        assert pod.status.phase == "Running", f"El pod {pod.metadata.name} no esta en estado 'Running'."
diff --git a/tests/test_metric_visualizer.py b/tests/test_metric_visualizer.py
index 21fbb90..df49019 100644
--- a/tests/test_metric_visualizer.py
+++ b/tests/test_metric_visualizer.py
@@ -1,18 +1,11 @@
 import pytest
-from scripts.metric_collector import metric_collector
+from pathlib import Path
 from scripts.metric_collector import metric_visualizer
 
 
 root_dir = metric_visualizer.find_root_dir("Proyecto7-PC4")
 
 
-def setup_module(module):
-    # Se ejecuta una vez antes de cualquier test en este archivo
-    print("\n[setup] Ejecutando dependencias...")
-    metric_collector.main()  # si aplica
-    metric_visualizer.main()  # o alguna función que prepare el entorno
-
-
 @pytest.mark.xfail(reason="El directorio no existe")
 def test_not_fing_root_dir():
     assert metric_visualizer.find_root_dir("non_existent_dir")
@@ -22,7 +15,7 @@ def test_find_root_dir():
     root = metric_visualizer.find_root_dir("Proyecto7-PC4")
     assert root.is_dir()
 
-
+@pytest.mark.xfail(reason="El directorio no existe")
 def test_clean_raw_metrics_pods():
     pod_path = root_dir / "metrics" / "pods" / "default_metrics.csv"
     df = metric_visualizer.clean_raw_metrics_pods(pod_path)
